{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be09787",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    129\u001b[39m rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Process frame for hands\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m hand_results = \u001b[43mhands\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Process frame for face\u001b[39;00m\n\u001b[32m    135\u001b[39m face_results = face_mesh.process(rgb_frame)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[39m, in \u001b[36mHands.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    133\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[33;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7,\n",
    "    max_num_hands=1\n",
    ")\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "# Load images\n",
    "original_img = cv2.imread(r\"C:\\Users\\farah\\Downloads\\original.jpg\")\n",
    "thumbs_up_img = cv2.imread(r\"C:\\Users\\farah\\Downloads\\thumbs_up.jpg\")\n",
    "pointing_img = cv2.imread(r\"C:\\Users\\farah\\Downloads\\pointing.jpg\")\n",
    "annoyed_img = cv2.imread(r\"C:\\Users\\farah\\Downloads\\annoyed.jpg\")\n",
    "peace_img = cv2.imread(r\"C:\\Users\\farah\\Downloads\\peace.jpg\")\n",
    "tongue_img = cv2.imread(r\"C:\\Users\\farah\\Downloads\\download.jpg\")\n",
    "closed_eye_img = cv2.imread(r\"C:\\Users\\farah\\Downloads\\closed eye.jpg\")\n",
    "\n",
    "if original_img is None or thumbs_up_img is None or pointing_img is None or annoyed_img is None or peace_img is None or tongue_img is None or closed_eye_img is None:\n",
    "    print(\"Error: one or more images not found\")\n",
    "    print(\"Required images: original.jpg, thumbs_up.jpg, pointing.jpg, annoyed.jpg, peace.jpg, download.jpg, closed eye.jpg\")\n",
    "    exit()\n",
    "\n",
    "# Get dimensions from original image\n",
    "height, width = original_img.shape[:2]\n",
    "\n",
    "# Resize all images to match\n",
    "thumbs_up_img = cv2.resize(thumbs_up_img, (width, height))\n",
    "pointing_img = cv2.resize(pointing_img, (width, height))\n",
    "annoyed_img = cv2.resize(annoyed_img, (width, height))\n",
    "peace_img = cv2.resize(peace_img, (width, height))\n",
    "tongue_img = cv2.resize(tongue_img, (width, height))\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "current_img = original_img.copy()\n",
    "\n",
    "def count_fingers(hand_landmarks):\n",
    "    \"\"\"Count extended fingers\"\"\"\n",
    "    finger_count = 0\n",
    "    \n",
    "    # Finger tip indices: thumb, index, middle, ring, pinky\n",
    "    tips = [4, 8, 12, 16, 20]\n",
    "    pips = [3, 6, 10, 14, 18]\n",
    "    \n",
    "    # Check each finger\n",
    "    for tip, pip in zip(tips, pips):\n",
    "        tip_y = hand_landmarks.landmark[tip].y\n",
    "        pip_y = hand_landmarks.landmark[pip].y\n",
    "        \n",
    "        # Finger is extended if tip is above pip\n",
    "        if tip_y < pip_y:\n",
    "            finger_count += 1\n",
    "    \n",
    "    return finger_count\n",
    "\n",
    "def detect_thumbs_up(hand_landmarks):\n",
    "    \"\"\"Detect thumbs up gesture\"\"\"\n",
    "    # Thumb tip should be above thumb base\n",
    "    thumb_up = hand_landmarks.landmark[4].y < hand_landmarks.landmark[3].y\n",
    "    \n",
    "    # All other fingers should be down (curled)\n",
    "    index_down = hand_landmarks.landmark[8].y > hand_landmarks.landmark[6].y\n",
    "    middle_down = hand_landmarks.landmark[12].y > hand_landmarks.landmark[10].y\n",
    "    ring_down = hand_landmarks.landmark[16].y > hand_landmarks.landmark[14].y\n",
    "    pinky_down = hand_landmarks.landmark[20].y > hand_landmarks.landmark[18].y\n",
    "    \n",
    "    return thumb_up and index_down and middle_down and ring_down and pinky_down\n",
    "\n",
    "def detect_pointing(hand_landmarks):\n",
    "    \"\"\"Detect pointing gesture (only index finger up)\"\"\"\n",
    "    # Index finger should be extended\n",
    "    index_up = hand_landmarks.landmark[8].y < hand_landmarks.landmark[6].y\n",
    "    \n",
    "    # All other fingers should be down\n",
    "    thumb_down = hand_landmarks.landmark[4].y > hand_landmarks.landmark[3].y\n",
    "    middle_down = hand_landmarks.landmark[12].y > hand_landmarks.landmark[10].y\n",
    "    ring_down = hand_landmarks.landmark[16].y > hand_landmarks.landmark[14].y\n",
    "    pinky_down = hand_landmarks.landmark[20].y > hand_landmarks.landmark[18].y\n",
    "    \n",
    "    return index_up and thumb_down and middle_down and ring_down and pinky_down\n",
    "\n",
    "def detect_peace_sign(hand_landmarks):\n",
    "    \"\"\"Detect peace sign (index and middle fingers up)\"\"\"\n",
    "    # Index and middle fingers should be extended\n",
    "    index_up = hand_landmarks.landmark[8].y < hand_landmarks.landmark[6].y\n",
    "    middle_up = hand_landmarks.landmark[12].y < hand_landmarks.landmark[10].y\n",
    "    \n",
    "    # Ring and pinky should be down\n",
    "    ring_down = hand_landmarks.landmark[16].y > hand_landmarks.landmark[14].y\n",
    "    pinky_down = hand_landmarks.landmark[20].y > hand_landmarks.landmark[18].y\n",
    "    \n",
    "    return index_up and middle_up and ring_down and pinky_down\n",
    "\n",
    "def detect_mouth_open(face_landmarks):\n",
    "    \"\"\"Detect if mouth is open wide (tongue out)\"\"\"\n",
    "    # Upper lip landmark: 13\n",
    "    # Lower lip landmark: 14\n",
    "    upper_lip = face_landmarks.landmark[13]\n",
    "    lower_lip = face_landmarks.landmark[14]\n",
    "    \n",
    "    # Calculate distance between upper and lower lip\n",
    "    mouth_distance = abs(upper_lip.y - lower_lip.y)\n",
    "    \n",
    "    # If mouth is open wide (threshold 0.03)\n",
    "    return mouth_distance > 0.03\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Flip frame for mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process frame for hands\n",
    "    hand_results = hands.process(rgb_frame)\n",
    "    \n",
    "    # Process frame for face\n",
    "    face_results = face_mesh.process(rgb_frame)\n",
    "    \n",
    "    finger_count = 0\n",
    "    gesture_name = \"None\"\n",
    "    mouth_open = False\n",
    "    \n",
    "    # Check for tongue out first (highest priority)\n",
    "    if face_results.multi_face_landmarks:\n",
    "        for face_landmarks in face_results.multi_face_landmarks:\n",
    "            if detect_mouth_open(face_landmarks):\n",
    "                current_img = tongue_img.copy()\n",
    "                gesture_name = \"Tongue Out\"\n",
    "                mouth_open = True\n",
    "                break\n",
    "    \n",
    "    # Check hand gestures if mouth is not open\n",
    "    if not mouth_open and hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks on frame\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS\n",
    "            )\n",
    "            \n",
    "            # Count fingers\n",
    "            finger_count = count_fingers(hand_landmarks)\n",
    "            \n",
    "            # Check for thumbs up\n",
    "            if detect_thumbs_up(hand_landmarks):\n",
    "                current_img = thumbs_up_img.copy()\n",
    "                gesture_name = \"Thumbs Up\"\n",
    "            # Check for peace sign\n",
    "            elif detect_peace_sign(hand_landmarks):\n",
    "                current_img = peace_img.copy()\n",
    "                gesture_name = \"Peace Sign\"\n",
    "            # Check for pointing\n",
    "            elif detect_pointing(hand_landmarks):\n",
    "                current_img = pointing_img.copy()\n",
    "                gesture_name = \"Pointing\"\n",
    "            # Check for open hand (5 fingers)\n",
    "            elif finger_count >= 5:\n",
    "                current_img = annoyed_img.copy()\n",
    "                gesture_name = \"Annoyed\"\n",
    "            else:\n",
    "                current_img = original_img.copy()\n",
    "                gesture_name = \"Original\"\n",
    "    elif not mouth_open:\n",
    "        current_img = original_img.copy()\n",
    "        gesture_name = \"Original\"\n",
    "    \n",
    "    # Overlay camera frame on image\n",
    "    frame_small = cv2.resize(frame, (width // 3, height // 3))\n",
    "    x_offset = width - frame_small.shape[1] - 10\n",
    "    y_offset = 10\n",
    "    current_img[y_offset:y_offset + frame_small.shape[0],\n",
    "                x_offset:x_offset + frame_small.shape[1]] = frame_small\n",
    "    \n",
    "    # Display info\n",
    "    cv2.putText(current_img, f\"Gesture: {gesture_name}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(current_img, f\"Fingers: {finger_count}\", (10, 70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    # Show result\n",
    "    cv2.imshow(\"Monster Control\", current_img)\n",
    "    \n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n",
    "face_mesh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
